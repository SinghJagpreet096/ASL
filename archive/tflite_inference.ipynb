{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jagpreetsingh/ML_Projects/projects/test/ASL/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import nbformat\n",
    "import tensorflow as tf\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "\n",
    "## defining the model \n",
    "interpreter = tf.lite.Interpreter(\"src/model.tflite\")\n",
    "found_signatures = list(interpreter.get_signature_list().keys())\n",
    "prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
    "\n",
    " # Add ordinally Encoded Sign (assign number to each sign name)\\\n",
    "train = pd.read_csv('src/data/train.csv.zip')\n",
    "train['sign_ord'] = train['sign'].astype('category').cat.codes\n",
    "\n",
    "# Dictionaries to translate sign <-> ordinal encoded sign\n",
    "SIGN2ORD = train[['sign', 'sign_ord']].set_index('sign').squeeze().to_dict()\n",
    "ORD2SIGN = train[['sign_ord', 'sign']].set_index('sign_ord').squeeze().to_dict()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ROWS_PER_FRAME = 543  # number of landmarks per frame\n",
    "\n",
    "def load_relevant_data_subset(data):\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = data[data_columns]\n",
    "    # data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)\n",
    "\n",
    "# pq_file = \"output.parquet\"\n",
    "def prediction_func(data):\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    prediction = prediction_fn(inputs=xyz_np)\n",
    "    pred = prediction['outputs'].argmax()\n",
    "    sign = ORD2SIGN[pred]\n",
    "    prediction_conf = prediction['outputs'][pred]\n",
    "    return sign, prediction_conf\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import nbformat\n",
    "import tensorflow as tf\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "\n",
    "def create_frame_landmark_df(results, frame, xyz):\n",
    "    xyz_skel = xyz[['type','landmark_index']].drop_duplicates().reset_index(drop=True).copy()\n",
    "\n",
    "    face = pd.DataFrame()\n",
    "    pose = pd.DataFrame()\n",
    "    left_hand = pd.DataFrame()\n",
    "    right_hand = pd.DataFrame()\n",
    "    if results.face_landmarks:\n",
    "        for i, point in enumerate(results.face_landmarks.landmark):\n",
    "            face.loc[i, ['x','y','z']] = [point.x, point.y, point.z]\n",
    "    if results.pose_landmarks:\n",
    "        for i , point in enumerate(results.pose_landmarks.landmark):\n",
    "            pose.loc[i, ['x', 'y', 'z']] = [point.x, point.y, point.z]\n",
    "    if results.left_hand_landmarks:\n",
    "        for i, point in enumerate(results.left_hand_landmarks.landmark):\n",
    "            left_hand.loc[i, ['x','y','z']] = [point.x, point.y, point.z]\n",
    "    if results.right_hand_landmarks:\n",
    "        for i, point in enumerate(results.right_hand_landmarks.landmark):\n",
    "            right_hand.loc[i, ['x', 'y', 'z']] = [point.x, point.y, point.z]   \n",
    "    face = face.reset_index() \\\n",
    "        .rename(columns={'index':'landmark_index'}) \\\n",
    "            .assign(type='face')\n",
    "    pose = pose.reset_index() \\\n",
    "        .rename(columns={'index':'landmark_index'}) \\\n",
    "            .assign(type='pose')\n",
    "    left_hand = left_hand.reset_index() \\\n",
    "        .rename(columns={'index':'landmark_index'}) \\\n",
    "            .assign(type='left_hand')\n",
    "    right_hand = right_hand.reset_index() \\\n",
    "        .rename(columns={'index':'landmark_index'}) \\\n",
    "            .assign(type='right_hand')\n",
    "\n",
    "\n",
    "    landmarks = pd.concat([face,pose,right_hand,left_hand]).reset_index(drop=True)\n",
    "    landmarks = xyz_skel.merge(landmarks, on=['type','landmark_index'], how='left')\n",
    "    landmarks = landmarks.assign(frame=frame)\n",
    "    return landmarks\n",
    "\n",
    "\n",
    "\n",
    "# For webcam input:\n",
    "def do_capture_loop(xyz):\n",
    "    all_landmarks = pd.DataFrame()\n",
    "    # count = 0\n",
    "    cap = cv2.VideoCapture(1)\n",
    "    with mp_holistic.Holistic(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as holistic:\n",
    "        frame = 0\n",
    "        while cap.isOpened():\n",
    "            frame+=1\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                # If loading a video, use 'break' instead of 'continue'.\n",
    "                continue\n",
    "\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "            image.flags.writeable = False\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = holistic.process(image)\n",
    "            \n",
    "            # mp_hands = mp.solutions.hands\n",
    "\n",
    "            \n",
    "        ## create landmarks dataframe from results\n",
    "            landmarks = create_frame_landmark_df(results,frame,xyz)\n",
    "            all_landmarks = pd.concat([landmarks]).reset_index(drop=True)\n",
    "            ## load data from output parquet\n",
    "            # xyz_np = load_relevant_data_subset(all_landmarks)\n",
    "            # prediction, confidence = prediction_func(all_landmarks)\n",
    "        \n",
    "        # Draw landmark annotation on the image.\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.face_landmarks,\n",
    "                mp_holistic.FACEMESH_CONTOURS,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp_drawing_styles\n",
    "                .get_default_face_mesh_contours_style())\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.pose_landmarks,\n",
    "                mp_holistic.POSE_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing_styles\n",
    "                .get_default_pose_landmarks_style())\n",
    "            #Flip the image horizontally for a selfie-view display.\n",
    "            \n",
    "            # text = prediction_func(pq_file)\n",
    "            # print(text)\n",
    "            # draw_predictions(image,text)\n",
    "            # cv2.rectangle(frame, (x, y - text_height - 5), (x + text_width, y), (0, 0, 0), -1)\n",
    "            # cv2.putText(image, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            cv2.imshow('MediaPipe Holistic', cv2.flip(image, 1))\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == 27:\n",
    "                break\n",
    "    # cap.release()\n",
    "    return  results,landmarks\n",
    "\n",
    "\n",
    "# pd.concat(landmarks).reset_index(drop=True).to_parquet('output.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_file_sample = \"src/data/100015657.parquet\"\n",
    "xyz = pd.read_parquet(pq_file_sample)\n",
    "hands, landmarks = do_capture_loop(xyz)\n",
    "# print(sign)\n",
    "# print(f\"Sing:{sign} with {prediction_conf} confidence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if hands.left_hand_landmarks or hands.right_hand_landmarks:\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_hands = hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks_hands is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>landmark_index</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>frame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>left_hand</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>left_hand</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>left_hand</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>left_hand</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>left_hand</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>left_hand</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>left_hand</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>left_hand</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>left_hand</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>left_hand</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>left_hand</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>left_hand</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>left_hand</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>left_hand</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>left_hand</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>left_hand</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>left_hand</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>left_hand</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>left_hand</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>left_hand</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>left_hand</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          type  landmark_index   x   y   z  frame\n",
       "468  left_hand               0 NaN NaN NaN     19\n",
       "469  left_hand               1 NaN NaN NaN     19\n",
       "470  left_hand               2 NaN NaN NaN     19\n",
       "471  left_hand               3 NaN NaN NaN     19\n",
       "472  left_hand               4 NaN NaN NaN     19\n",
       "473  left_hand               5 NaN NaN NaN     19\n",
       "474  left_hand               6 NaN NaN NaN     19\n",
       "475  left_hand               7 NaN NaN NaN     19\n",
       "476  left_hand               8 NaN NaN NaN     19\n",
       "477  left_hand               9 NaN NaN NaN     19\n",
       "478  left_hand              10 NaN NaN NaN     19\n",
       "479  left_hand              11 NaN NaN NaN     19\n",
       "480  left_hand              12 NaN NaN NaN     19\n",
       "481  left_hand              13 NaN NaN NaN     19\n",
       "482  left_hand              14 NaN NaN NaN     19\n",
       "483  left_hand              15 NaN NaN NaN     19\n",
       "484  left_hand              16 NaN NaN NaN     19\n",
       "485  left_hand              17 NaN NaN NaN     19\n",
       "486  left_hand              18 NaN NaN NaN     19\n",
       "487  left_hand              19 NaN NaN NaN     19\n",
       "488  left_hand              20 NaN NaN NaN     19"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks.query(\"type=='left_hand'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if landmarks.query(\"type=='left_hand'\")['x'].isnull().:\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to open file at /Users/jagpreetsingh/ML_Projects/projects/test/ASL/hand_landmarker.task",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m base_options \u001b[39m=\u001b[39m python\u001b[39m.\u001b[39mBaseOptions(model_asset_path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhand_landmarker.task\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m options \u001b[39m=\u001b[39m vision\u001b[39m.\u001b[39mHandLandmarkerOptions(base_options\u001b[39m=\u001b[39mbase_options,\n\u001b[1;32m      9\u001b[0m                                 num_hands\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m detector \u001b[39m=\u001b[39m vision\u001b[39m.\u001b[39;49mHandLandmarker\u001b[39m.\u001b[39;49mcreate_from_options(options)\n\u001b[1;32m     12\u001b[0m \u001b[39m# STEP 3: Load the input image.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m image \u001b[39m=\u001b[39m mp\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mcreate_from_file(\u001b[39m\"\u001b[39m\u001b[39mhand.jpg\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/ML_Projects/projects/test/ASL/venv/lib/python3.10/site-packages/mediapipe/tasks/python/vision/hand_landmarker.py:364\u001b[0m, in \u001b[0;36mHandLandmarker.create_from_options\u001b[0;34m(cls, options)\u001b[0m\n\u001b[1;32m    342\u001b[0m   options\u001b[39m.\u001b[39mresult_callback(\n\u001b[1;32m    343\u001b[0m       hand_landmarks_detection_result,\n\u001b[1;32m    344\u001b[0m       image,\n\u001b[1;32m    345\u001b[0m       timestamp\u001b[39m.\u001b[39mvalue \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m _MICRO_SECONDS_PER_MILLISECOND,\n\u001b[1;32m    346\u001b[0m   )\n\u001b[1;32m    348\u001b[0m task_info \u001b[39m=\u001b[39m _TaskInfo(\n\u001b[1;32m    349\u001b[0m     task_graph\u001b[39m=\u001b[39m_TASK_GRAPH_NAME,\n\u001b[1;32m    350\u001b[0m     input_streams\u001b[39m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    362\u001b[0m     task_options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m    363\u001b[0m )\n\u001b[0;32m--> 364\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    365\u001b[0m     task_info\u001b[39m.\u001b[39;49mgenerate_graph_config(\n\u001b[1;32m    366\u001b[0m         enable_flow_limiting\u001b[39m=\u001b[39;49moptions\u001b[39m.\u001b[39;49mrunning_mode\n\u001b[1;32m    367\u001b[0m         \u001b[39m==\u001b[39;49m _RunningMode\u001b[39m.\u001b[39;49mLIVE_STREAM\n\u001b[1;32m    368\u001b[0m     ),\n\u001b[1;32m    369\u001b[0m     options\u001b[39m.\u001b[39;49mrunning_mode,\n\u001b[1;32m    370\u001b[0m     packets_callback \u001b[39mif\u001b[39;49;00m options\u001b[39m.\u001b[39;49mresult_callback \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    371\u001b[0m )\n",
      "File \u001b[0;32m~/ML_Projects/projects/test/ASL/venv/lib/python3.10/site-packages/mediapipe/tasks/python/vision/core/base_vision_task_api.py:70\u001b[0m, in \u001b[0;36mBaseVisionTaskApi.__init__\u001b[0;34m(self, graph_config, running_mode, packet_callback)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39melif\u001b[39;00m packet_callback:\n\u001b[1;32m     66\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     67\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mThe vision task is in image or video mode, a user-defined result \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     68\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mcallback should not be provided.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     69\u001b[0m   )\n\u001b[0;32m---> 70\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_runner \u001b[39m=\u001b[39m _TaskRunner\u001b[39m.\u001b[39;49mcreate(graph_config, packet_callback)\n\u001b[1;32m     71\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_running_mode \u001b[39m=\u001b[39m running_mode\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to open file at /Users/jagpreetsingh/ML_Projects/projects/test/ASL/hand_landmarker.task"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/93/mr9vk9s104n71qvsfvrkf4800000gn/T/ipykernel_96501/498617807.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mlandmarks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"(type=='right_hand')\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML_Projects/projects/test/ASL/venv/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1464\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1466\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1467\u001b[0m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1468\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1469\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "if landmarks.query(\"(type=='right_hand')\")[['x']] is Null:\n",
    "    \n",
    "    print('*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NULL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 2\u001b[0m a \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m NULL\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NULL' is not defined"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "a is not NULL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'police'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_func(data)\n",
    "# load_relevant_data_subset(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61, 61)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(data)/ROWS_PER_FRAME),int(len(parquet_data)/ROWS_PER_FRAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[        nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan],\n",
       "        ...,\n",
       "        [        nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan]],\n",
       "\n",
       "       [[ 0.57347399,  0.55271643, -0.04534879],\n",
       "        [ 0.57167715,  0.47773084, -0.06143662],\n",
       "        [ 0.57177788,  0.50622123, -0.03806645],\n",
       "        ...,\n",
       "        [        nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan]],\n",
       "\n",
       "       [[ 0.57099456,  0.55596769, -0.04227388],\n",
       "        [ 0.56821048,  0.4816947 , -0.06146888],\n",
       "        [ 0.56885695,  0.50883514, -0.03679752],\n",
       "        ...,\n",
       "        [        nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.5871107 ,  0.65902555, -0.0354271 ],\n",
       "        [ 0.58743012,  0.5757944 , -0.07314876],\n",
       "        [ 0.58684832,  0.59814155, -0.0376913 ],\n",
       "        ...,\n",
       "        [ 0.63700253,  1.00829601, -0.07482404],\n",
       "        [ 0.65511686,  1.06052947, -0.05912786],\n",
       "        [ 0.65031457,  1.10699296, -0.04280748]],\n",
       "\n",
       "       [[ 0.5854556 ,  0.65473747, -0.0370019 ],\n",
       "        [ 0.58436567,  0.57229978, -0.07622433],\n",
       "        [ 0.58456522,  0.59445047, -0.03950353],\n",
       "        ...,\n",
       "        [        nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan]],\n",
       "\n",
       "       [[ 0.58130854,  0.65662664, -0.03700722],\n",
       "        [ 0.58035958,  0.57318377, -0.07540815],\n",
       "        [ 0.58068633,  0.59541446, -0.03906767],\n",
       "        ...,\n",
       "        [ 0.64355296,  1.00813413, -0.05265764],\n",
       "        [ 0.66675842,  1.05760539, -0.03782064],\n",
       "        [ 0.66967243,  1.10111392, -0.02448437]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['face', 0, 1, nan, nan, nan],\n",
       "       ['face', 1, 1, nan, nan, nan],\n",
       "       ['face', 2, 1, nan, nan, nan],\n",
       "       ...,\n",
       "       ['right_hand', 18, 61, 0.6435529589653015, 1.008134126663208,\n",
       "        -0.05265763774514198],\n",
       "       ['right_hand', 19, 61, 0.6667584180831909, 1.0576053857803345,\n",
       "        -0.037820637226104736],\n",
       "       ['right_hand', 20, 61, 0.669672429561615, 1.1011139154434204,\n",
       "        -0.024484368041157722]], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_frame_landmark_df(results, frame, xyz):\n",
    "#     xyz_skel = xyz[['type','landmark_index']].drop_duplicates().reset_index(drop=True).copy()\n",
    "\n",
    "#     face = pd.DataFrame()\n",
    "#     pose = pd.DataFrame()\n",
    "#     left_hand = pd.DataFrame()\n",
    "#     right_hand = pd.DataFrame()\n",
    "#     if results.face_landmarks:\n",
    "#         for i, point in enumerate(results.face_landmarks.landmark):\n",
    "#             face.loc[i, ['x','y','z']] = [point.x, point.y, point.z]\n",
    "#     if results.pose_landmarks:\n",
    "#         for i , point in enumerate(results.pose_landmarks.landmark):\n",
    "#             pose.loc[i, ['x', 'y', 'z']] = [point.x, point.y, point.z]\n",
    "#     if results.left_hand_landmarks:\n",
    "#         for i, point in enumerate(results.left_hand_landmarks.landmark):\n",
    "#             left_hand.loc[i, ['x','y','z']] = [point.x, point.y, point.z]\n",
    "#     if results.right_hand_landmarks:\n",
    "#         for i, point in enumerate(results.right_hand_landmarks.landmark):\n",
    "#             right_hand.loc[i, ['x', 'y', 'z']] = [point.x, point.y, point.z]   \n",
    "#     face = face.reset_index() \\\n",
    "#         .rename(columns={'index':'landmark_index'}) \\\n",
    "#             .assign(type='face')\n",
    "#     pose = pose.reset_index() \\\n",
    "#         .rename(columns={'index':'landmark_index'}) \\\n",
    "#             .assign(type='pose')\n",
    "#     left_hand = left_hand.reset_index() \\\n",
    "#         .rename(columns={'index':'landmark_index'}) \\\n",
    "#             .assign(type='left_hand')\n",
    "#     right_hand = right_hand.reset_index() \\\n",
    "#         .rename(columns={'index':'landmark_index'}) \\\n",
    "#             .assign(type='right_hand')\n",
    "\n",
    "\n",
    "#     landmarks = pd.concat([face,pose,right_hand,left_hand]).reset_index(drop=True)\n",
    "#     landmarks = xyz_skel.merge(landmarks, on=['type','landmark_index'], how='left')\n",
    "#     landmarks = landmarks.assign(frame=frame)\n",
    "#     return landmarks\n",
    "\n",
    "\n",
    "\n",
    "# def do_capture_loop(xyz):\n",
    "#     all_landmarks = []\n",
    "#     cap = cv2.VideoCapture(1)\n",
    "#     with mp_holistic.Holistic(\n",
    "#         min_detection_confidence=0.5,\n",
    "#         min_tracking_confidence=0.5) as holistic:\n",
    "#         frame = 0\n",
    "#         while cap.isOpened():\n",
    "#             frame+=1\n",
    "#             success, image = cap.read()\n",
    "#             if not success:\n",
    "#                 print(\"Ignoring empty camera frame.\")\n",
    "#                 # If loading a video, use 'break' instead of 'continue'.\n",
    "#                 continue\n",
    "\n",
    "#         # To improve performance, optionally mark the image as not writeable to\n",
    "#         # pass by reference.\n",
    "#             image.flags.writeable = False\n",
    "#             image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#             results = holistic.process(image)\n",
    "\n",
    "#         ## create landmarks dataframe from results\n",
    "#             landmarks = create_frame_landmark_df(results,frame,xyz)\n",
    "#             all_landmarks.append(landmarks)\n",
    "        \n",
    "#         # Draw landmark annotation on the image.\n",
    "#             image.flags.writeable = True\n",
    "#             image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "#             mp_drawing.draw_landmarks(\n",
    "#                 image,\n",
    "#                 results.face_landmarks,\n",
    "#                 mp_holistic.FACEMESH_CONTOURS,\n",
    "#                 landmark_drawing_spec=None,\n",
    "#                 connection_drawing_spec=mp_drawing_styles\n",
    "#                 .get_default_face_mesh_contours_style())\n",
    "#             mp_drawing.draw_landmarks(\n",
    "#                 image,\n",
    "#                 results.pose_landmarks,\n",
    "#                 mp_holistic.POSE_CONNECTIONS,\n",
    "#                 landmark_drawing_spec=mp_drawing_styles\n",
    "#                 .get_default_pose_landmarks_style())\n",
    "#             #Flip the image horizontally for a selfie-view display.\n",
    "            \n",
    "#             # text = prediction_func(pq_file)\n",
    "#             # print(text)\n",
    "#             # draw_predictions(image,text)\n",
    "#             # cv2.rectangle(frame, (x, y - text_height - 5), (x + text_width, y), (0, 0, 0), -1)\n",
    "#             # cv2.putText(image, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "#             cv2.imshow('MediaPipe Holistic', cv2.flip(image, 1))\n",
    "\n",
    "#             if cv2.waitKey(5) & 0xFF == 27:\n",
    "#                 break\n",
    "#     # cap.release()\n",
    "#     return all_landmarks\n",
    "# pq_file_sample = \"train_landmark_files/16069/100015657.parquet\"\n",
    "# xyz = pd.read_parquet(pq_file_sample)\n",
    "# landmarks = do_capture_loop(xyz)\n",
    "# pd.concat(landmarks).reset_index(drop=True).to_parquet('output.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
