{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import nbformat\n",
    "import tensorflow as tf\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ROWS_PER_FRAME = 543  # number of landmarks per frame\n",
    "\n",
    "def load_relevant_data_subset(data):\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = data[data_columns]\n",
    "    # data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 112\u001b[0m\n\u001b[1;32m    110\u001b[0m pq_file_sample \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain_landmark_files/16069/100015657.parquet\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m xyz \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_parquet(pq_file_sample)\n\u001b[0;32m--> 112\u001b[0m data,text \u001b[39m=\u001b[39m do_capture_loop(xyz)\n\u001b[1;32m    113\u001b[0m \u001b[39mprint\u001b[39m(text)\n\u001b[1;32m    114\u001b[0m \u001b[39m# pd.concat(landmarks).reset_index(drop=True).to_parquet('output.parquet')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[56], line 73\u001b[0m, in \u001b[0;36mdo_capture_loop\u001b[0;34m(xyz)\u001b[0m\n\u001b[1;32m     71\u001b[0m     image\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(image, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m---> 73\u001b[0m     results \u001b[39m=\u001b[39m holistic\u001b[39m.\u001b[39;49mprocess(image)\n\u001b[1;32m     75\u001b[0m \u001b[39m## create landmarks dataframe from results\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     landmarks \u001b[39m=\u001b[39m create_frame_landmark_df(results,frame,xyz)\n",
      "File \u001b[0;32m~/ML_Projects/projects/test/ASL/venv/lib/python3.10/site-packages/mediapipe/python/solutions/holistic.py:160\u001b[0m, in \u001b[0;36mHolistic.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess\u001b[39m(\u001b[39mself\u001b[39m, image: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NamedTuple:\n\u001b[1;32m    137\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Processes an RGB image and returns the pose landmarks, left and right hand landmarks, and face landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m   results \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mprocess(input_data\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m'\u001b[39;49m: image})\n\u001b[1;32m    161\u001b[0m   \u001b[39mif\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks:  \u001b[39m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[39mfor\u001b[39;00m landmark \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks\u001b[39m.\u001b[39mlandmark:  \u001b[39m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[0;32m~/ML_Projects/projects/test/ASL/venv/lib/python3.10/site-packages/mediapipe/python/solution_base.py:365\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    359\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    360\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    361\u001b[0m         stream\u001b[39m=\u001b[39mstream_name,\n\u001b[1;32m    362\u001b[0m         packet\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    363\u001b[0m                                  data)\u001b[39m.\u001b[39mat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_simulated_timestamp))\n\u001b[0;32m--> 365\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph\u001b[39m.\u001b[39;49mwait_until_idle()\n\u001b[1;32m    366\u001b[0m \u001b[39m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39m# output stream names.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m solution_outputs \u001b[39m=\u001b[39m collections\u001b[39m.\u001b[39mnamedtuple(\n\u001b[1;32m    369\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mSolutionOutputs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_stream_type_info\u001b[39m.\u001b[39mkeys())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import nbformat\n",
    "import tensorflow as tf\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "\n",
    "def create_frame_landmark_df(results, frame, xyz):\n",
    "    xyz_skel = xyz[['type','landmark_index']].drop_duplicates().reset_index(drop=True).copy()\n",
    "\n",
    "    face = pd.DataFrame()\n",
    "    pose = pd.DataFrame()\n",
    "    left_hand = pd.DataFrame()\n",
    "    right_hand = pd.DataFrame()\n",
    "    if results.face_landmarks:\n",
    "        for i, point in enumerate(results.face_landmarks.landmark):\n",
    "            face.loc[i, ['x','y','z']] = [point.x, point.y, point.z]\n",
    "    if results.pose_landmarks:\n",
    "        for i , point in enumerate(results.pose_landmarks.landmark):\n",
    "            pose.loc[i, ['x', 'y', 'z']] = [point.x, point.y, point.z]\n",
    "    if results.left_hand_landmarks:\n",
    "        for i, point in enumerate(results.left_hand_landmarks.landmark):\n",
    "            left_hand.loc[i, ['x','y','z']] = [point.x, point.y, point.z]\n",
    "    if results.right_hand_landmarks:\n",
    "        for i, point in enumerate(results.right_hand_landmarks.landmark):\n",
    "            right_hand.loc[i, ['x', 'y', 'z']] = [point.x, point.y, point.z]   \n",
    "    face = face.reset_index() \\\n",
    "        .rename(columns={'index':'landmark_index'}) \\\n",
    "            .assign(type='face')\n",
    "    pose = pose.reset_index() \\\n",
    "        .rename(columns={'index':'landmark_index'}) \\\n",
    "            .assign(type='pose')\n",
    "    left_hand = left_hand.reset_index() \\\n",
    "        .rename(columns={'index':'landmark_index'}) \\\n",
    "            .assign(type='left_hand')\n",
    "    right_hand = right_hand.reset_index() \\\n",
    "        .rename(columns={'index':'landmark_index'}) \\\n",
    "            .assign(type='right_hand')\n",
    "\n",
    "\n",
    "    landmarks = pd.concat([face,pose,right_hand,left_hand]).reset_index(drop=True)\n",
    "    landmarks = xyz_skel.merge(landmarks, on=['type','landmark_index'], how='left')\n",
    "    landmarks = landmarks.assign(frame=frame)\n",
    "    return landmarks\n",
    "\n",
    "\n",
    "# For webcam input:\n",
    "def do_capture_loop(xyz):\n",
    "    all_landmarks = []\n",
    "    count = 0\n",
    "    cap = cv2.VideoCapture(1)\n",
    "    with mp_holistic.Holistic(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as holistic:\n",
    "        frame = 0\n",
    "        while cap.isOpened():\n",
    "            frame+=1\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                # If loading a video, use 'break' instead of 'continue'.\n",
    "                continue\n",
    "\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "            image.flags.writeable = False\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = holistic.process(image)\n",
    "\n",
    "        ## create landmarks dataframe from results\n",
    "            landmarks = create_frame_landmark_df(results,frame,xyz)\n",
    "            all_landmarks.append(landmarks)\n",
    "            text = prediction_func(landmarks)\n",
    "        \n",
    "        # Draw landmark annotation on the image.\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.face_landmarks,\n",
    "                mp_holistic.FACEMESH_CONTOURS,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp_drawing_styles\n",
    "                .get_default_face_mesh_contours_style())\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.pose_landmarks,\n",
    "                mp_holistic.POSE_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing_styles\n",
    "                .get_default_pose_landmarks_style())\n",
    "            #Flip the image horizontally for a selfie-view display.\n",
    "            \n",
    "            # text = prediction_func(pq_file)\n",
    "            # print(text)\n",
    "            # draw_predictions(image,text)\n",
    "            # cv2.rectangle(frame, (x, y - text_height - 5), (x + text_width, y), (0, 0, 0), -1)\n",
    "            # cv2.putText(image, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            cv2.imshow('MediaPipe Holistic', cv2.flip(image, 1))\n",
    "\n",
    "            if cv2.waitKey(5) & 0xFF == 27:\n",
    "                break\n",
    "    # cap.release()\n",
    "    return pd.concat(all_landmarks).reset_index(drop=True), text\n",
    "pq_file_sample = \"train_landmark_files/16069/100015657.parquet\"\n",
    "xyz = pd.read_parquet(pq_file_sample)\n",
    "data,text = do_capture_loop(xyz)\n",
    "print(text)\n",
    "# pd.concat(landmarks).reset_index(drop=True).to_parquet('output.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['x','y','z']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pq_file = \"output.parquet\"\n",
    "def prediction_func(data):\n",
    "\n",
    "    ## defining the model \n",
    "    interpreter = tf.lite.Interpreter(\"model.tflite\")\n",
    "    found_signatures = list(interpreter.get_signature_list().keys())\n",
    "    prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
    "\n",
    "    \n",
    "    # Add ordinally Encoded Sign (assign number to each sign name)\\\n",
    "    train = pd.read_csv('train.csv.zip')\n",
    "    train['sign_ord'] = train['sign'].astype('category').cat.codes\n",
    "\n",
    "    # Dictionaries to translate sign <-> ordinal encoded sign\n",
    "    SIGN2ORD = train[['sign', 'sign_ord']].set_index('sign').squeeze().to_dict()\n",
    "    ORD2SIGN = train[['sign_ord', 'sign']].set_index('sign_ord').squeeze().to_dict()\n",
    "\n",
    "    ## load data from output parquet\n",
    "    xyz_np = load_relevant_data_subset(data)\n",
    "    prediction = prediction_fn(inputs=xyz_np)\n",
    "    sign = prediction['outputs'].argmax()\n",
    "    return ORD2SIGN[sign]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'police'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_func(data)\n",
    "# load_relevant_data_subset(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61, 61)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(data)/ROWS_PER_FRAME),int(len(parquet_data)/ROWS_PER_FRAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[        nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan],\n",
       "        ...,\n",
       "        [        nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan]],\n",
       "\n",
       "       [[ 0.57347399,  0.55271643, -0.04534879],\n",
       "        [ 0.57167715,  0.47773084, -0.06143662],\n",
       "        [ 0.57177788,  0.50622123, -0.03806645],\n",
       "        ...,\n",
       "        [        nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan]],\n",
       "\n",
       "       [[ 0.57099456,  0.55596769, -0.04227388],\n",
       "        [ 0.56821048,  0.4816947 , -0.06146888],\n",
       "        [ 0.56885695,  0.50883514, -0.03679752],\n",
       "        ...,\n",
       "        [        nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.5871107 ,  0.65902555, -0.0354271 ],\n",
       "        [ 0.58743012,  0.5757944 , -0.07314876],\n",
       "        [ 0.58684832,  0.59814155, -0.0376913 ],\n",
       "        ...,\n",
       "        [ 0.63700253,  1.00829601, -0.07482404],\n",
       "        [ 0.65511686,  1.06052947, -0.05912786],\n",
       "        [ 0.65031457,  1.10699296, -0.04280748]],\n",
       "\n",
       "       [[ 0.5854556 ,  0.65473747, -0.0370019 ],\n",
       "        [ 0.58436567,  0.57229978, -0.07622433],\n",
       "        [ 0.58456522,  0.59445047, -0.03950353],\n",
       "        ...,\n",
       "        [        nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan],\n",
       "        [        nan,         nan,         nan]],\n",
       "\n",
       "       [[ 0.58130854,  0.65662664, -0.03700722],\n",
       "        [ 0.58035958,  0.57318377, -0.07540815],\n",
       "        [ 0.58068633,  0.59541446, -0.03906767],\n",
       "        ...,\n",
       "        [ 0.64355296,  1.00813413, -0.05265764],\n",
       "        [ 0.66675842,  1.05760539, -0.03782064],\n",
       "        [ 0.66967243,  1.10111392, -0.02448437]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['face', 0, 1, nan, nan, nan],\n",
       "       ['face', 1, 1, nan, nan, nan],\n",
       "       ['face', 2, 1, nan, nan, nan],\n",
       "       ...,\n",
       "       ['right_hand', 18, 61, 0.6435529589653015, 1.008134126663208,\n",
       "        -0.05265763774514198],\n",
       "       ['right_hand', 19, 61, 0.6667584180831909, 1.0576053857803345,\n",
       "        -0.037820637226104736],\n",
       "       ['right_hand', 20, 61, 0.669672429561615, 1.1011139154434204,\n",
       "        -0.024484368041157722]], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_frame_landmark_df(results, frame, xyz):\n",
    "#     xyz_skel = xyz[['type','landmark_index']].drop_duplicates().reset_index(drop=True).copy()\n",
    "\n",
    "#     face = pd.DataFrame()\n",
    "#     pose = pd.DataFrame()\n",
    "#     left_hand = pd.DataFrame()\n",
    "#     right_hand = pd.DataFrame()\n",
    "#     if results.face_landmarks:\n",
    "#         for i, point in enumerate(results.face_landmarks.landmark):\n",
    "#             face.loc[i, ['x','y','z']] = [point.x, point.y, point.z]\n",
    "#     if results.pose_landmarks:\n",
    "#         for i , point in enumerate(results.pose_landmarks.landmark):\n",
    "#             pose.loc[i, ['x', 'y', 'z']] = [point.x, point.y, point.z]\n",
    "#     if results.left_hand_landmarks:\n",
    "#         for i, point in enumerate(results.left_hand_landmarks.landmark):\n",
    "#             left_hand.loc[i, ['x','y','z']] = [point.x, point.y, point.z]\n",
    "#     if results.right_hand_landmarks:\n",
    "#         for i, point in enumerate(results.right_hand_landmarks.landmark):\n",
    "#             right_hand.loc[i, ['x', 'y', 'z']] = [point.x, point.y, point.z]   \n",
    "#     face = face.reset_index() \\\n",
    "#         .rename(columns={'index':'landmark_index'}) \\\n",
    "#             .assign(type='face')\n",
    "#     pose = pose.reset_index() \\\n",
    "#         .rename(columns={'index':'landmark_index'}) \\\n",
    "#             .assign(type='pose')\n",
    "#     left_hand = left_hand.reset_index() \\\n",
    "#         .rename(columns={'index':'landmark_index'}) \\\n",
    "#             .assign(type='left_hand')\n",
    "#     right_hand = right_hand.reset_index() \\\n",
    "#         .rename(columns={'index':'landmark_index'}) \\\n",
    "#             .assign(type='right_hand')\n",
    "\n",
    "\n",
    "#     landmarks = pd.concat([face,pose,right_hand,left_hand]).reset_index(drop=True)\n",
    "#     landmarks = xyz_skel.merge(landmarks, on=['type','landmark_index'], how='left')\n",
    "#     landmarks = landmarks.assign(frame=frame)\n",
    "#     return landmarks\n",
    "\n",
    "\n",
    "\n",
    "# def do_capture_loop(xyz):\n",
    "#     all_landmarks = []\n",
    "#     cap = cv2.VideoCapture(1)\n",
    "#     with mp_holistic.Holistic(\n",
    "#         min_detection_confidence=0.5,\n",
    "#         min_tracking_confidence=0.5) as holistic:\n",
    "#         frame = 0\n",
    "#         while cap.isOpened():\n",
    "#             frame+=1\n",
    "#             success, image = cap.read()\n",
    "#             if not success:\n",
    "#                 print(\"Ignoring empty camera frame.\")\n",
    "#                 # If loading a video, use 'break' instead of 'continue'.\n",
    "#                 continue\n",
    "\n",
    "#         # To improve performance, optionally mark the image as not writeable to\n",
    "#         # pass by reference.\n",
    "#             image.flags.writeable = False\n",
    "#             image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#             results = holistic.process(image)\n",
    "\n",
    "#         ## create landmarks dataframe from results\n",
    "#             landmarks = create_frame_landmark_df(results,frame,xyz)\n",
    "#             all_landmarks.append(landmarks)\n",
    "        \n",
    "#         # Draw landmark annotation on the image.\n",
    "#             image.flags.writeable = True\n",
    "#             image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "#             mp_drawing.draw_landmarks(\n",
    "#                 image,\n",
    "#                 results.face_landmarks,\n",
    "#                 mp_holistic.FACEMESH_CONTOURS,\n",
    "#                 landmark_drawing_spec=None,\n",
    "#                 connection_drawing_spec=mp_drawing_styles\n",
    "#                 .get_default_face_mesh_contours_style())\n",
    "#             mp_drawing.draw_landmarks(\n",
    "#                 image,\n",
    "#                 results.pose_landmarks,\n",
    "#                 mp_holistic.POSE_CONNECTIONS,\n",
    "#                 landmark_drawing_spec=mp_drawing_styles\n",
    "#                 .get_default_pose_landmarks_style())\n",
    "#             #Flip the image horizontally for a selfie-view display.\n",
    "            \n",
    "#             # text = prediction_func(pq_file)\n",
    "#             # print(text)\n",
    "#             # draw_predictions(image,text)\n",
    "#             # cv2.rectangle(frame, (x, y - text_height - 5), (x + text_width, y), (0, 0, 0), -1)\n",
    "#             # cv2.putText(image, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "#             cv2.imshow('MediaPipe Holistic', cv2.flip(image, 1))\n",
    "\n",
    "#             if cv2.waitKey(5) & 0xFF == 27:\n",
    "#                 break\n",
    "#     # cap.release()\n",
    "#     return all_landmarks\n",
    "# pq_file_sample = \"train_landmark_files/16069/100015657.parquet\"\n",
    "# xyz = pd.read_parquet(pq_file_sample)\n",
    "# landmarks = do_capture_loop(xyz)\n",
    "# pd.concat(landmarks).reset_index(drop=True).to_parquet('output.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
